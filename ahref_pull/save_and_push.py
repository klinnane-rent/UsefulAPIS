import json import pandas as pdfrom datetime import datetimedata = json.load(open('new_lost_rent.json'))df = pd.DataFrame(data['refdomains'])df['target'] = 'rent'appended_df = dfdomains = [ 'rentals', 'apartmentguide', 'rentcafe', 'apartments', 'forrent','zillow',  'hotpads', 'redfin', 'apartmentlist', 'realtor']for domain in domains:    data = json.load(open('new_lost_{}.json'.format(domain)))    df = pd.DataFrame(data['refdomains'])    df['target'] = domain    appended_df = pd.concat([appended_df, df])appended_df['date'] = pd.to_datetime(appended_df['date'])appended_df.to_csv("new_lost_refdomains.csv", index=False)from google.cloud import bigqueryfrom google.oauth2 import service_account# Path to your service account key filekey_path = "../Documents/bigquery_info/bq_consumerproductpython_cred.json"# Construct a BigQuery client object with your credentialscredentials = service_account.Credentials.from_service_account_file(key_path)client = bigquery.Client(credentials=credentials, project=credentials.project_id)# Configure the load jobdataset_id = 'klinnane'table_id = 'ahrefs_referring_domain_gt_20'file_path = 'new_lost_refdomains.csv'dataset_ref = client.dataset(dataset_id)table_ref = dataset_ref.table(table_id)job_config = bigquery.LoadJobConfig()job_config.source_format = bigquery.SourceFormat.CSVjob_config.skip_leading_rows = 1job_config.autodetect = Truejob_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND# Load the modified file into BigQuerywith open('new_lost_refdomains.csv', "rb") as source_file:    job = client.load_table_from_file(source_file, table_ref, job_config=job_config)job.result()new_date = datetime.now().strftime("%Y-%m-%d")with open('last_pulled.txt', 'w') as file:    file.write(new_date)