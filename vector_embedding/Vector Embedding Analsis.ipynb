{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187415ce-a3c5-43df-b550-031726c4a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "\n",
    "vectored_data = pd.read_csv('frog_vectored_rent.csv')\n",
    "\n",
    "\n",
    "def scann_search(dataset:np.ndarray, queries: np.ndarray, n_neighbors = 10, distance_measure = \"dot_product\", num_leaves = 2000, num_leaves_to_search = 100):\n",
    "  normalized_dataset = dataset / np.linalg.norm(dataset, axis=1)[:, np.newaxis]\n",
    "\n",
    "  searcher = scann.scann_ops_pybind.builder(normalized_dataset, n_neighbors, distance_measure).tree(\n",
    "      num_leaves=num_leaves, num_leaves_to_search=num_leaves_to_search, training_sample_size=250000).score_ah(\n",
    "      2, anisotropic_quantization_threshold=0.2).reorder(100).build()\n",
    "\n",
    "  return searcher\n",
    "\n",
    "def convert_scann_arrays_to_urls(arrays: np.array, df: pd.DataFrame,column):\n",
    "    results = []\n",
    "    for arr in arrays:\n",
    "      results.append(df.iloc[arr.flatten()][column].tolist())\n",
    "    return results\n",
    "\n",
    "siteDf = siteDf[siteDf['openAiEmbeddings'].isna() == False]\n",
    "siteDf['openAiEmbeddingsAsFloats'] = siteDf['openAiEmbeddings'].str.split(',')\n",
    "siteDf['openAiEmbeddingsAsFloats'] = siteDf['openAiEmbeddingsAsFloats'].apply(lambda x: np.float64(x))\n",
    "siteDf['EmbeddingLength'] = siteDf['openAiEmbeddingsAsFloats'].apply(lambda x: x.size)\n",
    "\n",
    "if siteDf['EmbeddingLength'].unique().size == 1:\n",
    "  d = siteDf['EmbeddingLength'].unique() #Number of dimensions for each value\n",
    "else:\n",
    "  print('Dimensionality reduction required to make all arrays the same size.')\n",
    "\n",
    "dataset = np.vstack(vectored_data['openAiEmbeddingsAsFloats'].values)\n",
    "queries = dataset\n",
    "\n",
    "siteSearcher = scann_search(dataset, queries)\n",
    "siteSearcher.serialize(index_directory+'/site_scann_index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e2ae58-a601-4fde-9e6f-1e987209199c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings for SEMRUSH keyword to comapare 1-1 with our data\n",
    "\n",
    "\n",
    "# Function to get embeddings and flatten them for SCANN\n",
    "def get_openai_embeddings(keyword):\n",
    "    response = openai.embeddings.create(\n",
    "        input=keyword,\n",
    "        model=\"text-embedding-3-small\"  # Make sure to use the same embeddings as Screaming Frog\n",
    "    )\n",
    "    # Extract and flatten the embedding\n",
    "    embedding_vector = response.data[0].embedding\n",
    "    return np.array(embedding_vector).flatten()\n",
    "\n",
    "semrushFile = 'semrush_keywords.csv'\n",
    "keywordDf = read_file(semrushFile, 'CSV')\n",
    "display(keywordDf)\n",
    "\n",
    "# Loop through the DataFrame and get embeddings for each keyword\n",
    "embeddings = []\n",
    "for keyword in keywordDf['Keyword']:\n",
    "    embeddings.append(get_openai_embeddings(keyword))\n",
    "\n",
    "keywordDf['embeddings'] = embeddings\n",
    "\n",
    "# Create a temporary DataFrame for Excel output with embeddings converted to strings\n",
    "tempDf = keywordDf.copy()\n",
    "tempDf['embeddings'] = tempDf['embeddings'].apply(lambda x: str(x))\n",
    "tempDf.to_excel('semrush-embeddings.xlsx', index=False) # Save with embeddings as strings\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(keywordDf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb6d91-d533-48d3-bab3-a479ddff4e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword-to-keyword and keyword-to-page relationships \n",
    "# Wherever the highest ranking URL does not match the current landing page, thatâ€™s a linking opportunity for optimization.\n",
    "\n",
    "queries = np.vstack(keywordDf['embeddings'].values) #Stacking all individual embeddings vertically into matrix\n",
    "\n",
    "kwSearcher = scann_search(dataset, queries) # dataset is the same as before\n",
    "nearest_neighbors = kwSearcher.search_batched(queries, final_num_neighbors=1)\n",
    "matched_urls = convert_scann_arrays_to_urls(nearest_neighbors, siteDf, 'Address')\n",
    "\n",
    "keywordDf['BestMatchURL'] = convert_scann_arrays_to_urls(neighbors, siteDf, 'Address')\n",
    "keywordDf['BestMatchURL'] = keywordDf['BestMatchURL'].apply(lambda x: x[:1][0])\n",
    "\n",
    "display(keywordDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0b4e39-7ba1-41ec-830d-66aa2e9a122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize embeddings\n",
    "def normalize_embeddings(embeddings):\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    return (embeddings / norms).tolist()  # Normalize and convert to list\n",
    "\n",
    "# Normalize the embeddings and convert them to lists for DataFrame storage\n",
    "keywordDf['NormalizedEmbeddings'] = normalize_embeddings(np.vstack(keywordDf['embeddings'].values))\n",
    "siteDf['NormalizedEmbeddings'] = normalize_embeddings(np.vstack(siteDf['OpenAI Embeddings 1ConvertedFloats'].values))\n",
    "\n",
    "# Function to calculate cosine similarity\n",
    "def cosine_similarity(embedding1, embedding2):\n",
    "    return np.dot(embedding1, embedding2)\n",
    "\n",
    "# Initialize a list to store cosine similarity results\n",
    "cosine_similarities = []\n",
    "relevance_values = []\n",
    "\n",
    "# Loop through each keyword to calculate cosine similarity with its corresponding URL in siteDf\n",
    "for index, row in keywordDf.iterrows():\n",
    "    keyword_url = row['URL']\n",
    "    keyword_embedding = row['NormalizedEmbeddings']  # This is now a list\n",
    "\n",
    "    # Find the corresponding URL in siteDf\n",
    "    if keyword_url in siteDf['Address'].values:\n",
    "        # Get the embedding for the matching URL, which is also stored as a list\n",
    "        url_embedding = siteDf.loc[siteDf['Address'] == keyword_url, 'NormalizedEmbeddings'].iloc[0]\n",
    "        # Convert list to numpy array for calculation\n",
    "        similarity = cosine_similarity(np.array(keyword_embedding), np.array(url_embedding))\n",
    "        relevance = similarity * 100\n",
    "    else:\n",
    "        similarity = None  # Set similarity to None if no matching URL is found\n",
    "        relevance = None\n",
    "\n",
    "    cosine_similarities.append(similarity)\n",
    "    relevance_values.append(relevance)\n",
    "\n",
    "# Store the cosine similarities in the keywordDf\n",
    "keywordDf['CosineSimilarity'] = cosine_similarities\n",
    "keywordDf['Relevance'] = relevance_values\n",
    "\n",
    "\n",
    "# Display or use the updated DataFrame\n",
    "print(keywordDf[['Keyword', 'URL', 'CosineSimilarity','Relevance']])\n",
    "\n",
    "keywordDf.to_excel('keyword-relevance.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee371d4b-220a-4cc2-a693-9c814a1e7829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal Linking \n",
    "\n",
    "\n",
    "# Search siteDf for keywords, return 10 neighbors per keyword\n",
    "\n",
    "queries = np.vstack(keywordDf['embeddings'].values) #Stacking all individual embeddings vertically into matrix\n",
    "\n",
    "kwSearcher = scann_search(dataset, queries) # dataset is the same as before\n",
    "neighbors, distances = siteSearcher.search_batched(queries, leaves_to_search = 150)\n",
    "\n",
    "nearest_neighbors = kwSearcher.search_batched(queries, final_num_neighbors=5)\n",
    "\n",
    "matched_urls = convert_scann_arrays_to_urls(nearest_neighbors, siteDf, 'Address')\n",
    "\n",
    "keywordDf['InternalLinkSuggestions'] = convert_scann_arrays_to_urls(neighbors, siteDf, 'Address')\n",
    "keywordDf['InternalLinkSuggestions'] = keywordDf['InternalLinkSuggestions'].apply(lambda x: x[1:])\n",
    "display(keywordDf)\n",
    "\n",
    "# Create a temporary DataFrame for Excel output with embeddings converted to strings\n",
    "tempDf = keywordDf.copy()\n",
    "tempDf['embeddings'] = tempDf['embeddings'].apply(lambda x: str(x))\n",
    "tempDf.to_excel('keyword-internal-link-mapping.xlsx', index=False) # Save with embeddings as strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2471391b-2f4d-42cc-83c0-e9bffd6ab37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra inclusion \n",
    "\n",
    "# Doing another crawl of screaming frogs with:: \n",
    "#//return seoSpider.data(document.body.innerText); //#\n",
    "# we can merge with original dataset to understand how often same topics are \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f208eba-5c82-4cb8-846c-b09f098e1c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_and_visualize_content(df, embeddings_col):\n",
    "\n",
    "    # Prepare data\n",
    "    df['Page Content'] = df['Page Content'].astype(str)\n",
    "    keywords = df['Page Content'].tolist()\n",
    "    embeddings = np.vstack(df[embeddings_col].tolist())  # Ensure embeddings are properly shaped\n",
    "    embeddings = normalize(embeddings)  # Normalize embeddings for cosine similarity\n",
    "\n",
    "    prompt = \"\"\"\n",
    "      I have topic that is described by the following keywords: [KEYWORDS]\n",
    "      I am attempting to categorize this topic as part of 2-4 word taxonomy label that encapsulates all the keywords.\n",
    "      Based on the above information, can you give a short taxonomy label of the topic? Just return the taxonomy label itself.\n",
    "      \"\"\"\n",
    "    client = openai.OpenAI(api_key=openai.api_key)\n",
    "    representation_model = OpenAI(client, model=\"gpt-3.5-turbo\", prompt=prompt,chat=True)\n",
    "    # Initialize BERTopic\n",
    "    topic_model = BERTopic(representation_model=representation_model,calculate_probabilities=True)\n",
    "\n",
    "    # Fit BERTopic\n",
    "    topics, probabilities = topic_model.fit_transform(keywords, embeddings)\n",
    "    df['topic'] = topics  # Adding topic numbers to the DataFrame\n",
    "\n",
    "    # Visualize the topics with t-SNE\n",
    "    print(\"Reducing dimensions for visualization...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, metric='euclidean')\n",
    "    reduced_embeddings = tsne.fit_transform(embeddings)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=topics, cmap='viridis', s=50, alpha=0.6)\n",
    "    plt.colorbar()\n",
    "    plt.title('Content Topics Visualization with t-SNE')\n",
    "    plt.xlabel('t-SNE Feature 1')\n",
    "    plt.ylabel('t-SNE Feature 2')\n",
    "    plt.show()\n",
    "\n",
    "    # Probability distribution visualization\n",
    "    min_probability = 0.01\n",
    "    if any(probabilities[0] > min_probability):\n",
    "        print(\"Visualizing topic probabilities...\")\n",
    "        fig = topic_model.visualize_distribution(probabilities[0], min_probability=min_probability)\n",
    "        fig.show()\n",
    "    else:\n",
    "        print(\"No topic probabilities above the threshold to visualize.\")\n",
    "\n",
    "    # Intertopic distance map\n",
    "    print(\"Visualizing intertopic distance map...\")\n",
    "    fig = topic_model.visualize_topics()\n",
    "    fig.show()\n",
    "\n",
    "    # Hierarchical clustering\n",
    "    print(\"Visualizing hierarchical clustering...\")\n",
    "    fig = topic_model.visualize_hierarchy()\n",
    "    fig.show()\n",
    "\n",
    "    # Extract and name topics\n",
    "    df['topic_name'] = df['topic'].apply(lambda x: topic_model.get_topic(x)[0][0] if topic_model.get_topic(x) else 'No dominant topic')\n",
    "\n",
    "    # Display DataFrame with topic names\n",
    "    display(df)\n",
    "\n",
    "    # Export the DataFrame with topic labels\n",
    "    df.to_excel('content-clusters-bertopic.xlsx', index=False)\n",
    "\n",
    "pageContentDf = read_file('ipr-content.xlsx', 'Excel')\n",
    "\n",
    "contentEmbeddingsDf = siteDf.merge(pageContentDf, on='Address', how='inner')\n",
    "#print(contentEmbeddingsDf)\n",
    "\n",
    "cluster_and_visualize_content(contentEmbeddingsDf, 'OpenAI Embeddings 1ConvertedFloats')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
